---
title: "Lab5-Text Analysis 101 Using R"
author: "Yongjun Zhang, Ph.D."
institute: "Department of Sociology and IACS, Stony Brook Unversity"
date: ""
output:
  rmdformats::readthedown:
    highlight: pygments
--- = =
---

```{=html}
<style type="text/css">
p{ /* Normal  */
   font-size: 18px;
}
body{ /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objects

This tutorial aims to introduce basic ways to preprocess texual data before we model data using R. We will cover:

1. How to read, clean, and transform text data

2. How to preprocess data such as tokenization, removing stop words, lemmatization, stemming, and representing words in R

3. How to get basic statistics from texts using lexicon methods

4. How to implement lda and stm in R.

In the previous tutorial, we have covered some basics about how to read and save files in R, how to recognize regEx, and how to use selenium to do webscraping. 

We were able to successfully scrape the BLM protest events dataset. You can access the dataset <https://yongjunzhang.com/files/css/blm-data.tsv>

Note that some of these codes in this lab tutorial came from previous ones. 

## Intro to Preprossing Textual Data with R

#### We need to load some packages for use

```{r}
require(pacman)
packages<-c("tidyverse","tidytext","quanteda","haven","readxl","here","knitr","stopwords")
p_load(packages,character.only = TRUE)
```

### Let us then use quanteda package do some text processing in R (FINALLY :))

Check here for Quanteda<https://quanteda.io/articles/pkgdown/quickstart.html>

> quanteda, Quantitative Analysis of Textual Data, is an R package for managing and analyzing textual data developed by Kenneth Benoit, Kohei Watanabe, and other contributors. 

> The package is designed for R users needing to apply natural language processing to texts, from documents to final analysis. Its capabilities match or exceed those provided in many end-user software applications, many of which are expensive and not open source. The package is therefore of great benefit to researchers, students, and other analysts with fewer financial resources. While using quanteda requires R programming knowledge, its API is designed to enable powerful, efficient analysis with a minimum of steps. By emphasizing consistent design, furthermore, quanteda lowers the barriers to learning and using NLP and quantitative text analysis even for proficient R programmers.

> You are also encourage to install several recommended packages, including readtext, spacyr, and quanteda.corpora.

In this part, we however use some new york times articles to run analysis. If you manage to obtain all protest articles, you can use these protest articles as well. If not, you can use the small sample of nyt dataset. It has title_doca, text, and title_proquest. The title_doca ALLOWs you to merge nyt articles with doca data.

Note that you can download the doca raw dataset from this link: <https://web.stanford.edu/group/collectiveaction/cgi-bin/drupal/>. Then you can merge doca data with nyt articles. Ideally you can treat doca dataset as your TRAINING dataset, and you can train some models to predict protest related outcomes.

```{r}
load("./doca_nyt.rdata")
```

#### Let us build a doca nyt corpus

Quanteda has a corpus constructor command corpus():
- a vector of character objects, for instance that you have already loaded into the workspace using other tools;
- a VCorpus corpus object from the tm package.
- a data.frame containing a text column and any other document-level metadata

.
```{r}
doca_nyt_corpus <- corpus(doca_nyt)  # build a new corpus from the texts
#summary(doca_nyt_corpus)
```

##### How a quanteda corpus works

> A corpus is designed to be a “library” of original documents that have been converted to plain, UTF-8 encoded text, and stored along with meta-data at the corpus level and at the document-level. We have a special name for document-level meta-data: docvars. These are variables or features that describe attributes of each document.

> A corpus is designed to be a more or less static container of texts with respect to processing and analysis. This means that the texts in corpus are not designed to be changed internally through (for example) cleaning or pre-processing steps, such as stemming or removing punctuation. Rather, texts can be extracted from the corpus as part of processing, and assigned to new objects, but the idea is that the corpus will remain as an original reference copy so that other analyses – for instance those in which stems and punctuation were required, such as analyzing a reading ease index – can be performed on the same corpus.

> To extract texts from a corpus, we use an extractor, called texts().

```{r}
as.character(doca_nyt_corpus)[2]
```

> Tokenize texts: To simply tokenize a text, quanteda provides a powerful command called tokens(). This produces an intermediate object, consisting of a list of tokens in the form of character vectors, where each element of the list corresponds to an input document.

```{r}
tokens(as.character(doca_nyt_corpus)[2], remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)
```

##### Constructing a document-feature matrix

Tokenizing texts is an intermediate option, and most users will want to skip straight to constructing a document-feature matrix. For this, we have a Swiss-army knife function, called dfm(), which performs tokenization and tabulates the extracted features into a matrix of documents by features. Unlike the conservative approach taken by tokens(), the dfm() function applies certain options by default, such as tolower() – a separate function for lower-casing texts – and removes punctuation. 

```{r}
# make a dfm
my_tokens <- tokens(doca_nyt_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)
my_dfm <- dfm(my_tokens) %>% 
  dfm_remove(stopwords("english")) %>% 
  dfm_wordstem()
my_dfm[, 1:5]
```
> Viewing the document-feature matrix: The dfm can be inspected in the Enviroment pane in RStudio, or by calling R’s View function. Calling plot on a dfm will display a wordcloud.

```{r}
topfeatures(my_dfm, 20)  # 20 top words
```

```{r}
library(quanteda.textplots)
set.seed(100)
textplot_wordcloud(my_dfm, min_count = 6, random_order = FALSE,
                   rotation = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
```


# Latent Dirichlet Allocation

> We will start with the simple latent dirichlet allocation with gibbs sampling. This part is adapted from [Ethen Liu](https://github.com/ethen8181/machine-learning/blob/master/clustering_old/topic_model/)'s intuitive demo to LDA. For detailed intuitive or tech intro, please check our lecture slides or Blei's article. You can also check here for [gibbs sampling](http://www.pnas.org/cgi/doi/10.1073/pnas.0307752101).

**Latent Dirichlet Allocation** (LDA) is a probabilistic topic modeling method that allows us to discover and annotate texts. The key assumptions are as follows (see Mohr and Bogdanov 2013) .

> Each document (text) within a corpus is viewed as a bag-of-words produced according to a mixture of themes that the author of the text intended to discuss. Each theme (or topic) is a distribution over all observed words in the corpus, such that words that are strongly associated with the document's dominant topics have a higher chance of being selected and placed in the document bag. Thus, the goal of topic modeling is to find the parameters of the LDA process that has likely generated the corpus.

Based on this week's reading (Blei 2012), we know that the topic distribution for each document is

$$ \theta \sim Dirichlet(\alpha) $$

Where $Dirichlet(\alpha)$ denotes the Dirichlet distribution for parameter $\alpha$.

The word distribution for each topic also modeled by a Dirichlet distribution with a different parameter $\eta$.

$$ \beta \sim Dirichlet(\eta) $$

Our goal is to estimate the $\theta$ and $\beta$ using observed words in documents. That being said, we are trying to understand which words are important for which topic and which topics are important for a particular document, respectively.

Note that the [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) is a probability distribution for parameters $\alpha$. Where $\alpha$ governs the concentration of the distribution. Sometimes people call this *concentration parameter* or *scaling parameter*. When $\alpha$ approaches 0, it means documents are concentrated in a few topics. So a higher value suggests that topics are more evenly distributed across the documents. This also applied to $\beta$ regarding topic-word.

We will use Gibbs sampling to compute the conditional probability specified in Blei's article (eq 2). Generally speaking, LDA is a generative model of word counts. We are interested in the conditional probability of hidden topic structure given the observed words in documents.

To simplify the demo process, we will use 10 short strings to represent 10 documents (Note that recent study shows that the length of document and the number of documents do influence our results. Just be careful about this). We deliberately get 5 sentences describing Chinese food and 5 sentences describing American football from Wikipedia.

Usually before running topic model, we need to normalize our texts as shown in our lecture (like tidy texts, removing stop words, white-spaces, etc.). We often use tidytext, tm, or [quanteda](https://tutorials.quanteda.io/) packages in R to preprocess the texts, but now let us stick to basic stuff. I strongly suggest you to take some time to read the quanteda tutorial.

```{r}
library(tidyverse)
raw_docs <- c(
	"Chinese cuisine is an important part of Chinese culture, which includes cuisine originating from the diverse regions of China, as well as from Overseas Chinese who have settled in other parts of the world.",
	"The preference for seasoning and cooking techniques of Chinese provinces depend on differences in historical background and ethnic groups.",
	"Chinese society greatly valued gastronomy, and developed an extensive study of the subject based on its traditional medical beliefs.",
	"There are a variety of styles of cooking in China, but Chinese chefs have classified eight regional cuisines according to their distinct tastes and local characteristics. ",
	"Based on the raw materials and ingredients used, the method of preparation and cultural differences, a variety of foods with different flavors and textures are prepared in different regions of the country. ",
	"American football, referred to as football in the United States and Canada and also known as gridiron,is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end",
	"American football evolved in the United States, originating from the sports of soccer and rugby. The first American football match was played on November 6, 1869, between two college teams, Rutgers and Princeton, using rules based on the rules of soccer at the time.",
	"American football is the most popular sport in the United States. The most popular forms of the game are professional and college football, with the other major levels being high school and youth football. ",
	"In football, the winner is the team that has scored more points at the end of the game. There are multiple ways to score in a football game. ",
	"Football games last for a total of 60 minutes in professional and college play and are divided into two halves of 30 minutes and four quarters of 15 minutes."
)

# lower cases and remove punctuation or double spaces
raw_docs <- str_remove_all(tolower(raw_docs),"[:punct:]")

# remove stop words
stopwords_regex = paste(stopwords::stopwords('en'), collapse = '\\b|\\b')
stopwords_regex = paste0('\\b', stopwords_regex, '\\b')
raw_docs <- str_replace_all(raw_docs, stopwords_regex, '')

# remove the most frequent words, chinese, american, football
raw_docs <- str_remove_all(raw_docs, "chinese|american|football")
raw_docs[[1]]

# let us squish our text, removing extra spaces
raw_docs <- str_squish(raw_docs)

# segmenting each work, similar to tokenization.
docs <- str_split(raw_docs, " ")
docs[[1]]

# get a vocabulary of unique words in our corpus
vocab <- unique(unlist(docs))

# represent strings using numerical numbers
# use the base match function match(x,table)
# If x[i] is found to equal table[j] then the value returned in the i-th position of the return value is j, for the smallest possible j. 
# for( i in 1:length(docs) ) {
# 	docs[[i]] <- match( docs[[i]], vocab )
# }
docs <- map(1:length(docs), ~ match(docs[[.]], vocab))
docs
```

In LDA, we have to specify the number of clusters (i.e., topics) first. Usually it was denoted by K. In this case, let us do 2.

If we recall correctly, in Blei's article, he described the generative process of LDA. It has several major steps.

```{r, out.width="",out.height="",fig.align='center', fig.cap='Blei 2012'}
knitr::include_graphics('figure1.png')

```

> Here, let us first go through each document and randomly assign each word in the document to one of the K topics. This is the topic assignment process. The right side of Blei's article in Figure 1.

> Then we create a **word-topic matrix**, which is the count of each word being assigned to each topic. And a **document-topic matrix**, which is the number of words assigned to each topic for each document.

```{r}

# cluster number 
K <- 2

# initialize count matrices 
# @wt : word-topic matrix 
wt <- matrix(0, nrow = K, ncol = length(vocab))
colnames(wt) <- vocab

# @ta : topic assignment list
ta <- map(docs, ~ rep(0, length(.))) 
names(ta) <- paste0("doc", 1:length(docs))

# @dt : counts correspond to the number of words assigned to each topic for each document
dt <- matrix(0, length(docs), K)

set.seed(2022)
for (d in 1:length(docs)) { 
	# randomly assign topic to word w
	for(w in 1:length(docs[[d]])) {
		ta[[d]][w] <- sample(1:K, 1) 

		# extract the topic index, word id and update the corresponding cell in the word-topic count matrix  
		ti <- ta[[d]][w]
		wi <- docs[[d]][w]
		wt[ti, wi] <- wt[ti, wi] + 1    
		# josh's comments- the initial value for wt[ti,wi] is 0, and now we update it to 1 because we assign a word to that topic. so the count of words increases to 1.
	}

	# count words in document d assigned to each topic t
  # Josh's comment-okay, dt is a container for topic-document count 
	for( t in 1:K ) {
		dt[d, t] <- sum( ta[[d]] == t )
	}
}

# randomly assigned topic to each word
print(ta)
print(wt)
print(dt)

```

> Notice that this random assignment gives you both the topic representations of all the documents and word distributions of all the topics (bad ones!!!). We need to improve this!! Optimize it!

> There are a couple of ways to do this. But we focus on Gibbs Sampling method that performs the following steps for a user-specified iteration:

> For each document d, go through each word w. Reassign a new topic to w from topic t with "the probability of word w given topic t" $\times$ "probability of topic t given document d", denoted by the following mathematical notations:

$$P( z_i = j \text{ }| \text{ } z_{-i}, w_i, d_i ) 
    \propto \frac{ C^{WT}_{w_ij} + \eta }{ \sum^W_{ w = 1 }C^{WT}_{wj} + W\eta } \times
      \frac{ C^{DT}_{d_ij} + \alpha }{ \sum^T_{ t = 1 }C^{DT}_{d_it} + T\alpha }$$

This formula is confusing! Let us talk bit by bit.

> Starting from the left side of the equal sign:

-   $P(z_i = j)$ : The probability that token i is assigned to topic j.
-   $z_{-i}$ : Represents topic assignments of all other tokens.
-   $w_i$ : Word (index) of the $i_{th}$ token.
-   $d_i$ : Document containing the $i_{th}$ token.

> For the right side of the equal sign:

-   $C^{WT}$ : Word-topic matrix, the `wt` matrix we generated.
-   $\sum^W_{ w = 1 }C^{WT}_{wj}$ : Total number of tokens (words) in each topic.
-   $C^{DT}$ : Document-topic matrix, the `dt` matrix we generated.
-   $\sum^T_{ t = 1 }C^{DT}_{d_it}$ : Total number of tokens (words) in document i.
-   $\eta$ : Parameter that sets the topic distribution for the words, the higher the more spread out the words will be across the specified number of topics (K).
-   $\alpha$ : Parameter that sets the topic distribution for the documents, the higher the more spread out the documents will be across the specified number of topics (K).
-   $W$ : Total number of words in the set of documents.
-   $T$ : Number of topics, equivalent of the K we defined earlier.

```{r}

# parameters 
alpha <- 1
eta <- 1

# initial topics assigned to the first word of the first document
# and its corresponding word id 
t0  <- ta[[1]][1]
wid <- docs[[1]][1]

# z_-i means that we do not include token w in our word-topic and document-topic count matrix when sampling for token w, only leave the topic assignments of all other tokens for document 1
dt[1, t0] <- dt[1, t0] - 1 
wt[t0, wid] <- wt[t0, wid] - 1

# Calculate left side and right side of equal sign
left  <- ( wt[, wid] + eta ) / ( rowSums(wt) + length(vocab) * eta )
right <- ( dt[1, ] + alpha ) / ( sum( dt[1, ] ) + K * alpha )

# draw new topic for the first word in the first document 
# The optional prob argument can be used to give a vector of weights for obtaining the elements of the vector being sampled. They need not sum to one, but they should be non-negative and not all zero.
t1 <- sample(1:K, 1, prob = left * right)
t1

```

> After the first iteration, the topic for the first word in the first document is updated to `r t1`.Just remember after drawing the new topic we need to update the topic assignment list with newly sampled topic for token w; re-increment the word-topic and document-topic count matrices with the new sampled topic for token w.
>
> We will use Ethen Liu's user-written function [`LDA1`][LDA] as a demo to run some interations, which takes the parameters of:

-   `docs` Document that have be converted to token (word) ids.
-   `vocab` Unique tokens (words) for all the document collection.
-   `K` Number of topic groups.
-   `alpha` and `eta` Distribution parameters as explained earlier.
-   `iterations` Number of iterations to run gibbs sampling to train our model.
-   Returns a list containing the final weight-topic count matrix `wt` and document-topic matrix `dt`.

```{r}

# define parameters
K <- 2 
alpha <- 1
eta <- 0.001
iterations <- 1000

source("LDA_functions.R")
set.seed(2022)
lda1 <- LDA1( docs = docs, vocab = vocab, 
			  K = K, alpha = alpha, eta = eta, iterations = iterations )
lda1

```

> After we're done with learning the topics for `r iterations` iterations, we can use the count matrices to obtain the word-topic distribution and document-topic distribution.
>
> To compute the probability of word given topic:

$$\beta_{ij} = \frac{C^{WT}_{ij} + \eta}{\sum^W_{ k = 1 }C^{WT}_{kj} + W\eta}$$

> Where $\beta_{ij}$ is the probability of word i for topic j.

```{r}

# topic probability of every word 
( beta <- ( lda1$wt + eta ) / ( rowSums(lda1$wt) + length(vocab) * eta ) )

```

$$\theta_{dj} = \frac{C^{DT}_{dj} + \alpha}{\sum^T_{ k = 1 }C^{DT}_{dk} + T\alpha}$$

Where $\theta_{dj}$ is the proportion of topic j in document d.

```{r}

# topic probability of every document
( theta <- ( lda1$dt + alpha ) / ( rowSums(lda1$dt) + K * alpha ) )

```

> Recall that LDA assumes that each document is a mixture of all topics, thus after computing the probability that each document belongs to each topic ( same goes for word & topic ) we can use this information to see which topic does each document belongs to and the more possible words that are associated with each topic. For more details on Gibbs Sampling, you can check Griffiths and Steyvers 2004 Finding Scientific topics.

```{r}

# topic assigned to each document, the one with the highest probability 
topic <- apply(theta, 1, which.max)

# possible words under each topic 
# sort the probability and obtain the user-specified number n
Terms <- function(beta, n) {
	term <- matrix(0, n, K)
	for( p in 1:nrow(beta) ) {
		term[, p] <- names( sort( beta[p, ], decreasing = TRUE )[1:n] )
	}
	return(term)
}
term <- Terms(beta = beta, n = 2)

```

> We specified that we wanted to see the top 2 terms associated with each topic. The following section prints out the original raw document, which is grouped into `r K` groups that we specified and words that are likely to go along with each topic.

```{r}

list( original_text = raw_docs[topic == 1], words = term[, 1] )
list( original_text = raw_docs[topic == 2], words = term[, 2] )

```

> The output tells us that the first topic seems to be discussing something about united states , while the second is something about food. It is still messy, not that intuitive. But at least it is a good starting point.

Now let us move to use the R library *topicmodels* to fit a LDA.

> Since the starting point of gibbs sampling is chosen randomly, thus it makes sense to discard the first few iteration ( also known as `burn-in` periods ). Due to the fact that they most likely do not correctly reflect the properties of distribution. And another parameter is `thin`, the number of iterations ommitted during the training. This serves to prevent correlations between samples during the iteration.
>
> We'll use the `LDA` function from the *topicmodels* library to implement gibbs sampling method on the same set of raw documents and print out the result for you to compare. Note that library has a default of value of 50 / K for $\alpha$ and 0.1 for $\eta$.

```{r, message=FALSE, warning=TRUE}

# load packages if not installed, using install.packages("topicmodels")
library(tm)
library(topicmodels)

# @burnin : number of omitted Gibbs iterations at beginning
# @thin : number of omitted in-between Gibbs iterations
docs1 <- Corpus( VectorSource(raw_docs) )
dtm <- DocumentTermMatrix(docs1)
# josh'cc- the input of LDA is a document-term matrix. You can use tm::DocumentTermMatrix to create it. Note you can also use tidytext package to do this. You can also use quanteda to do this. 
lda <- LDA( dtm, k = 2, method = "Gibbs", 
	   		control = list(seed = 2022, burnin = 500, thin = 100, iter = 4000) )

list( original_text = raw_docs[ topics(lda) == 1 ], words = terms(lda, 3)[, 1] )
list( original_text = raw_docs[ topics(lda) == 2 ], words = terms(lda, 3)[, 2] )

```

> Notice that after training the model for 4000 iterations and using a different $\alpha$ and $\eta$ value, we obtained a different document clustering result and different words that are more likely to associate with each topic. Since the goal here is to peform a clustering (unsupervised) method to unveil unknown patterns, the solutions will most likely differ as there is no such thing as a correct answer. We should try a range of different values of K to find the optimal topic grouping of the set of documents and see which result matches our intuition more.

# Structural Topic Model

> In this part we heavily rely on [stm's tutorial](http://structuraltopicmodel.com/) by Molly Roberts, Brandon Stewart and Dustin Tingley and an application by[Jula Silge] (<https://juliasilge.com/blog/sherlock-holmes-stm/>). We will go through their tutorial and show you how to do stm in R librabry stm.

Let us install stm first.

```{r, message=FALSE, warning=TRUE}
#library(devtools)
#install_github("bstewart/stm",dependencies=TRUE)
library(stm)
```

We use the data from stm tutorial, but we use our nyt dataset. <http://reports-archive.adm.cs.cmu.edu/anon/ml2010/CMU-ML-10-101.pdf>.

We need to merge with doca data to retrieve some meta data like publishing year of articles etc.

```{r}
library(haven)
doca <- read_dta("./final_data_v10.dta", encoding = "latin1")
# because doca is event-level data, we want article level data
doca_article <- doca %>% 
  transmute(title=tolower(title),
            rptmm,rptyy=as.numeric(rptyy),
            section_a=ifelse(section%in%c("a","A"),1,0),
            page,paragrph) %>% 
  distinct(title,.keep_all =TRUE) 

data <-  doca_nyt %>% 
  mutate(title_doca=tolower(title_doca)) %>% 
  left_join(doca_article,by=c("title_doca"="title"))
rm(doca_article)
rm(doca_nyt)
rm(doca)
```

Before we run topic models like lda, we need to preprocess data. STM provides several functions to automatically do stemming, stopwords removal, low frequency words removal, etc for you.

Of course, you can directly feed the created dtm to stm function as well. But let us use stm's processor first.

Here is the graph of stm processors:

```{r, out.width="",out.height="",fig.align='center', fig.cap='STM process'}
knitr::include_graphics('figure2.png')

```

Let us use the textProcessor to preprocess texts. Here is the function:

> textProcessor(documents, metadata = NULL, lowercase = TRUE, removestopwords = TRUE, removenumbers = TRUE, removepunctuation = TRUE, ucp = FALSE, stem = TRUE, wordLengths = c(3, Inf), sparselevel = 1, language = "en", verbose = TRUE, onlycharacter = FALSE, striphtml = FALSE, customstopwords = NULL, custompunctuation = NULL, v1 = FALSE)

```{r}
#Preprocessing
#stemming/stopword removal, etc.
#Josh-cc, if you don't know the details of a function, you can use ? to check the documentation of that function. ?textProcessor
processed <- textProcessor(data$text, metadata=data)
```

Let us use prepDocuments to perform several corpus manipulations including removing words and renumbering word indices. here is the function:

> prepDocuments(documents, vocab, meta = NULL, lower.thresh = 1, upper.thresh = Inf, subsample = NULL, verbose = TRUE)

```{r}
#before running prepDocuments, you can use plotRemoved function to check the appropriate threshold to remove words or documents.
#take a look at how many words and documents would be removed with different lower.thresholds !!! check Error: could not find function "plotRemoved"
plotRemoved(processed$documents, lower.thresh=seq(1,200, by=100))
```

```{r}
#structure and index for usage in the stm model. Verify no-missingness. can remove low frequency words using 'lower.thresh' option. See ?prepDocuments for more info
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thresh=1)
```

```{r}
#output will have object meta, documents, and vocab 
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```

Now, let us use stm function fit a stm model.

> The function takes sparse representation of a document-term matrix, an integer number of topics, and covariates and returns fitted model parameters. Covariates can be used in the prior for topic prevalence, in the prior for topical content or both.

> stm(documents, vocab, K, prevalence = NULL, content = NULL, data = NULL, init.type = c("Spectral", "LDA", "Random", "Custom"), seed = NULL, max.em.its = 500, emtol = 1e-05, verbose = TRUE, reportevery = 5, LDAbeta = TRUE, interactions = TRUE, ngroups = 1, model = NULL, gamma.prior = c("Pooled", "L1"), sigma.prior = 0, kappa.prior = c("L1", "Jeffreys"), control = list())

```{r}
#run an stm model using the 'out' data. 20 topics. Asking how prevalaence of topics varies across documents' meta data, including publishing year. !! option s(year) applies a spline normalization to year variable. We also include a variable section A, whether it is published in section a.

# max.em.its should be at least 100. We use 20 just as demo
stmFit <- stm(out$documents,out$vocab,
              K=20,
              prevalence =~ section_a+s(rptyy),
              max.em.its=20,
              data=out$meta,
              seed=2022,
              verbose = TRUE)

```

Like LDA, stm also need to specify the number of topics or themes (K) before fitting. Fortunately, stm provides a function selectModel to help you select the models with high likelihood values.

> selectModel(documents, vocab, K, prevalence = NULL, content = NULL, data = NULL, max.em.its = 100, verbose = TRUE, init.type = "LDA", emtol = 1e-05, seed = NULL, runs = 50, frexw = 0.7, net.max.em.its = 2, netverbose = FALSE, M = 10, N = NULL, to.disk = F, ...)

```{r}
#let STM help you compare a number of models side by side. It will keep the models that don't stink (i.e. that converge quickly) 
stmSelect <- selectModel(out$documents,
                         out$vocab,
                         K=20,
                         prevalence =~s(rptyy)+section_a,
                         max.em.its=20,# use 20 as a demo
                         data=meta,
                         runs=20,
                         seed=2022)

#plot the different models that make the cut along exclusivity and semantic coherence of their topics
plotModels(stmSelect)

```

```{r}
#the 3rd one looks best, so choose it and give it the name stmFit
stmFit<-stmSelect$runout[[3]] #choose the third model
```

Now it is time to interpret the stm model.

```{r}
###LIST OF TOP WORDS for topics 1, 7, & 10
labelTopics(stmFit, c(1, 7, 10))
```
Let us do a wordcloud, but I am not suggesting you to do this in your published research.
```{r}
###WORDCLOUD for a specified TOPIC
cloud(stmFit, topic=7)
```
Let us find some texts that are most representative for a particular topic using findThoughts function:

> Outputs most representative documents for a particular topic. Use this in order to get a better sense of the content of actual documents with a high topical content.

> findThoughts(model, texts = NULL, topics = NULL, n = 3,
  thresh = NULL, where = NULL, meta = NULL)

```{r}
#object 'thoughts1' contains 2 documents about topic 1. 'texts=shortdoc,' gives you just the first 250 words
data <- data %>% 
  mutate(shortdoc=text %>% 
           str_replace_all("\\n","") %>% 
           str_extract("^.{250}")) 

thoughts1 <- findThoughts(stmFit,
                          texts=data$shortdoc,
                          n=2,
                          topics=1)$docs[[1]]
#will show you the output
plotQuote(thoughts1, width=40, main="Topic 1")
```
Let use find more documents for topics
```{r}
#how about more documents for more of these topics?
thoughts7 <- findThoughts(stmFit, 
                          texts=data$shortdoc, 
                          n=2, 
                          topics=7)$docs[[1]]
thoughts10 <- findThoughts(stmFit, 
                           texts=data$shortdoc,
                           n=2, 
                           topics=10)$docs[[1]]
thoughts4 <- findThoughts(stmFit, 
                          texts=data$shortdoc,
                          n=2, 
                          topics=4)$docs[[1]]

#And in a 2X2 table? We like 2X2 tables!  --- Note: this command will force all remaining plots into a 2X2 table format
par(mfrow = c(2, 2),mar=c(.5,.5,1,.5)) 
plotQuote(thoughts1, width=50, main="Topic 1")
plotQuote(thoughts4, width=50, main="Topic 4")
plotQuote(thoughts7, width=50, main="Topic 7")
plotQuote(thoughts10, width=50, main="Topic 10")
```
Let us see PROPORTION OF EACH TOPIC in the entire CORPUS.

```{r}
## Just insert your STM output
plot.STM(stmFit, type="summary", n=5,xlim=c(0,.4))
```

Let us see how topics are correlated...
```{r}
##see GRAPHICAL NETWORK DISPLAY of how closely related topics are to one another, (i.e., how likely they are to appear in the same document) Requires 'igraph' package
mod.out.corr<-topicCorr(stmFit)
plot.topicCorr(mod.out.corr)
```

Let use see topical content by covariates
```{r}
##VISUALIZE DIFFERENCES BETWEEN TWO DIFFERENT TOPICS using the ,type="perspectives" option
plot.STM(stmFit,type="perspectives", topics=c(9, 10))
```

Let see how prevalence of topics varies across documents based on document covariates.

```{r}

###See CORRELATIONS BTWN METADATA & TOPIC PREVALANCE in documents
###First, must estimate an effect of the metadata covariates on topic prevalence in a document, so that we have anything to plot

#since we're preparing these coVariates by estimating their effects we call these estimated effects 'prep'
#we're estimating Effects across all 20 topics, 1:20. We're using 'section_a' and normalized 'rptyy,' using the topic model stmFit. 
#The meta data file we call meta. We are telling it to generate the model while accounting for all possible uncertainty. Note: when estimating effects of one covariate, others are held at their mean
prep <- estimateEffect(1:20 ~ section_a+s(rptyy),stmFit,meta=meta, uncertainty = "Global")

###See how PREVALENCE of TOPICS DIFFERS across VALUES of a CATEGORICAL COVARIATE  
plot.estimateEffect(prep, covariate = "section_a", topics = seq(1,20,1),
                    #topic model=stmFit. Method="difference" 
                    model=stmFit, method="difference",
                    #only using two values of covariate, and labeling them... assume we could do this with a non-binary covariate and just specify
                    cov.value1="0",cov.value2="1",
                    xlab="Not section a ... section a",
                    main="Effect of publishing in section a",
                    xlim=c(-.1,.1), labeltype = "custom")
```

```{r}
#See how PREVALENCE of TOPICS DIFFERS across VALUES of a CONTINUOUS COVARIATE
#plotting prep data on day variable, a continuous variable with a continous plot. focusing on topic 9.!
plot.estimateEffect(prep, "rptyy", method="continuous", topics=9, 
                    printlegend=FALSE, xaxt="n", xlab="Time(1965-1995)")
```

Let us see how words of the topics are emphasized differently across documents according to document covariates

```{r}
#### Instead of looking at how prevalent a topic is in a class of documents categorized by meta-data covariate... 
#### ... let's see how the words of the topic are emphasized differently in documents of each category of the covariate
##First, we we estimate a new stm. It's the same as the old one, including prevalence option, but we add in a content option
stmContent <- stm(out$documents,out$vocab,K=20,
                       prevalence =~ section_a+ s(rptyy), content=~section_a,
                       max.em.its=20, data=out$meta,seed=2022)
##Next, we plot using the ,type="perspectives" option to the plot.STM function
plot.STM(stmContent,type="perspectives", topics=3)
```

Now let us use supplement packages to visualize stm outputs.

> [stmprinter: Print multiple stm model dashboards to a pdf file for inspection](https://github.com/mikajoh/stmprinter). Beautiful automated reports from multiple stm runs.

> stminsights: A Shiny Application for Inspecting Structural Topic Models. A shiny GUI with beautiful graphics.

> themetagenomics: Exploring Thematic Structure and Predicted Functionality of 16s rRNA Amplicon Data. . STM for rRNA data.

> [tidystm: Extract (tidy) effects from estimateEffect](devtools::install_github(%22mikajoh/tidystm%22,%20dependencies%20=%20TRUE)). Makes it easy to make ggplot2 graphics for STM.

> stmgui: Shiny Application for Creating STM Models" . This is a Shiny GUI for running basic STM models.

> stmBrowser: An R Package for the Structural Topic Model Browser.'' This D3 visualization allows users to interactively explore the relationships between topics and the covariates estimated from the stm package in R.

> stmCorrViz: A Tool for Structural Topic Model Visualizations. This package uses D3 to generate an interactive hierarchical topic explorer.

```{r}
p_load(stmprinter,stminsights,themetagenomics, tidystm,stmgui,
            stmBrowser,stmCorrViz)
#devtools::install_github("mikajoh/stmprinter")
#devtools::install_github("mikajoh/tidystm", dependencies = TRUE)
#devtools::install_github("mroberts/stmBrowser",dependencies=TRUE)
```

Let us use stmBrowser to visualize our topic models. Check here for more details <https://github.com/mroberts/stmBrowser>.
The major function is stmBrowser.

```{r}
stmCorrViz::stmCorrViz(stmFit,
                       file_out = "./lab5-viz.html",
                       documents_raw=data$shortdoc)

```
You can check here for the [generated html visualization file](https://yongjunzhang.com/intro2css/assets/files/lab5-viz.html).


### THE END...