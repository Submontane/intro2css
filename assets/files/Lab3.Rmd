---
title: "Lab3 Machine Learning Basics in R"
author: "Yongjun Zhang, Ph.D."
institute: "Department of Sociology and IACS, Stony Brook Unversity"
date: ""
output:
  rmdformats::readthedown:
    highlight: pygments
--- = =
---

```{=html}
<style type="text/css">
p{ /* Normal  */
   font-size: 18px;
}
body{ /* Normal  */
   font-size: 18px;
}
td {  /* Table  */
   font-size: 14px;
}
h1 { /* Header 1 */
 font-size: 32px;
}
h2 { /* Header 2 */
 font-size: 26px;
}
h3 { /* Header 3 */
 font-size: 22px;
}
code.r{ /* Code block */
  font-size: 14px;
}
pre { /* Code block */
  font-size: 14px
}
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Learning Objects

This tutorial aims to introduce some basic ways to implement ml in R.

1. Learn how to use R caret package to train a supervised ml model.

2. Use SSA US baby names to train a model to predict gender


## Use caret package to train classic ML model

Max Kuhn has detailed documentation related to how to use caret package to train and tune models. You can check here for more details: <https://topepo.github.io/caret/>.

You can also get his book on Applied Predicting Modeling: <http://appliedpredictivemodeling.com/>

In this tutorial, we are going to rely on Max Kuhn's caret package to train a simple classifier to predict gender. 

> The caret package (short for Classification And REgression Training) has a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for: data splitting; pre-processing; feature selection; model tuning using resampling; variable importance estimation

Of course, there are other packages that you can use to train your own models, but I prefer using caret as it is one of the most popular in our community. 

Here our goal is to use data from SSA to train a gender classifier using name features.

### Data Preparation

> You can download ssa baby names via here <https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-data>

> I have also used the following codes to preprocess the data. I have generated aggregated name counts by gender and then define our target outcome as "female if over 50 of women use that name." I have also created four variables: first letter, first two letters, last letter, and laster two letters. You can directly use the RData file, <https://yongjunzhang.com/files/ssa_baby_names.RData>. 

> Of course, you can take a more rigorous way to define the gender of names.

```{r}
files <- list.files(path = "./names/", pattern = ".txt", full.names = TRUE)
files

# let us read the first file
library(tidyverse)
# let us say we want to define a read_us_baby function
readSsaBabyNames <- function(file, ...) {
  require(tidyverse)
  data <- read_csv(file, col_names = FALSE, show_col_types = FALSE) %>%
    mutate(year = str_replace_all(file, "[^0-9]", ""))
  colnames(data) <- c("names", "sex", "count", "year")
  return(data)
}

dat_year <- map_dfr(files, readSsaBabyNames)

# we only use names after 1970 and used by at least 10
dat_all <- dat_year %>%
  filter(year > 1970) %>%
  group_by(names, sex) %>%
  summarise(count = sum(count)) %>%
  # we only keep names used by at least 10
  filter(count > 10) %>%
  # reshape the long format to wide format
  pivot_wider(names_from = "sex", values_from = "count") %>%
  # replace NA with zeros
  replace_na(list(F = 0, M = 0)) %>%
  # create our target outcome female
  mutate(female = (F / (F + M) > .5) * 1) %>%
  # create a series of predictors
  mutate(
    flt1 = str_extract(names, "^.") %>% tolower(),
    flt2 = str_extract(names, "^.{2}") %>% tolower(),
    llt1 = str_extract(names, ".$") %>% tolower(),
    llt2 = str_extract(names, ".{2}$") %>% tolower()
  )

save(dat_year, dat_all, file = "./ssa_baby_names.RData")
```

### Split our data into train and test data

Before we further split our data, let us take a look at data first.

```{r}

top_letters <- dat_all %>% 
  group_by(flt1) %>% 
  summarise(fl1=n()) %>% 
  top_n(n=5) %>% 
  bind_cols(
    dat_all %>% 
      group_by(flt2) %>% 
      summarise(fl2=n()) %>% 
      top_n(n=5)
  ) %>% 
  bind_cols(
    dat_all %>% 
      group_by(llt1) %>% 
      summarise(ll1=n()) %>% 
      top_n(n=5)
  ) %>% 
  bind_cols(
    dat_all %>% 
      group_by(llt2) %>% 
      summarise(ll2=n()) %>% 
      top_n(n=5)
  )

knitr::kable(top_letters)

```

We are going to create a series of features based on first/first two and last/last two letters to predict gender of names. But in your own training, you should do some feature engineering and choose those most informative features.

```{r}
library(caret)
#create dummies for last letter, last two letters, fist letter, and first two letters of names
fllt_d=predict(dummyVars(~llt1+llt2+flt1+flt2,data=dat_all),newdata=dat_all)%>%as.data.frame()
# This will create a over 900 vars, and some of them are near zero variance, so we need to get rid of it.
#identify near zero variance variables
# nzv_test <- nearZeroVar(fllt_d,freqCut = 95/5, uniqueCut = 10, saveMetrics= TRUE,allowParallel = TRUE)
nzv <- nearZeroVar(fllt_d,freqCut = 95/5, uniqueCut = 10,allowParallel = TRUE)
fllt_nnzv<- fllt_d[, -nzv]
#create the full datasets with dummies
df=dat_all %>% 
  ungroup %>% 
  select(names,female) %>% 
  bind_cols(fllt_nnzv) %>% 
  filter(!is.na(female)) %>% 
  mutate(female = female==1)
```

Let us finally split our data into train and test

```{r}
inTrain <- createDataPartition(
  y = df$female,
  ## the outcome data are needed
  p = .75,
  ## The percentage of data in the
  ## training set
  list = FALSE
)

train <- df[ inTrain,]
test  <- df[-inTrain,]

nrow(train)
#> [1] 157
nrow(test)
library(gdata)
keep(dat_all,df,train,test,sure=TRUE)

```

### Let us train a xgboost model to predict gender

XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. Here is a good explanation <https://xgboost.readthedocs.io/en/stable/tutorials/model.html>.

You can check caret package's boosting methods here <https://topepo.github.io/caret/train-models-by-tag.html#boosting>. Of course, you can fine-tune hyper-parameters, but we use the default ones.

```{r}

# K folds cross validation
# try parallel computing
library(doParallel)
library(xgboost)
cl <- makePSOCKcluster(3)
registerDoParallel(cl)

grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

train_control <- caret::trainControl(
  method = "cv", #cross validation
  number = 3, # we do 10 cv
  verboseIter = FALSE, 
  allowParallel = TRUE 
)

xgb_base <- caret::train(
  female~., 
  data=train %>%  dplyr::select(-c(names)),
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)

stopCluster(cl)

save(xgb_base,file="./xgb_base.RData")

```

### Let us check model performance, get the comfusion matrix first

```{r}
#load("./xgb_base.RData")
# check cf matrix
xgb_base
confusionMatrix(xgb_base)

```


### Let us check the most informative features


```{r}
xgbImp <- varImp(xgb_base, scale = TRUE)
plot(xgbImp, top = 5)
```

### Test model performance on test set

```{r}

# predict name_remove
test_df <- test %>% dplyr::select(-c(names,female))
test_pred <- predict(xgb_base, newdata = test_df) %>% tibble()
colnames(test_pred) <- "xgb_female"

data <- test %>% select(names,female) %>% bind_cols(test_pred)

data %>% 
  write_csv("./test_pred.csv",na="")

# check confusion matrix

confusionMatrix(test_pred$xgb_female,test$female)
```

## Code Challenge 1

> Replicate the ML predicting baby names' gender using top 5 most frequent first/last and first-two/last two letters instead of all dummies

> Try other models, including support vector machine, penalized logit model, and naive bayes; You can also try random forest, but it takes a while to finish.

> Generate your final prediction based on svm, logit, and nb methods (using the majority vote)

> Save a csv file and report the confusion matrix

> You codes should be written in Rmarkdown file and genearte a pdf file.

## Optional-Using Keras to train a neural network to classify handwritten digits.

You can get the whole tutorial via <https://tensorflow.rstudio.com/guide/keras/>

Keras is a high-level neural networks API developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Keras has the following key features:

- Allows the same code to run on CPU or on GPU, seamlessly.

- User-friendly API which makes it easy to quickly prototype deep learning models.

- Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.

- Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc. This means that Keras is appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.

- See the main Keras website at https://keras.io for additional information on the project.

```{r}
#First, install the keras R package with:
#install.packages("keras")
#install_keras()
#or install the development version with:
#devtools::install_github("rstudio/keras")
```

### MNIST Example
We can learn the basics of Keras by walking through a simple example: recognizing handwritten digits from the MNIST dataset. MNIST consists of 28 x 28 grayscale images of handwritten digits.The dataset also includes labels for each image, telling us which digit it is. For example, the labels for the above images are 5, 0, 4, and 1.

### PREPARING THE DATA
The MNIST dataset is included with Keras and can be accessed using the dataset_mnist() function. Here we load the dataset then create variables for our test and training data:

```{r}
library(keras)
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y
```

The x data is a 3-d array (images,width,height) of grayscale values . To prepare the data for training we convert the 3-d arrays into matrices by reshaping width and height into a single dimension (28x28 images are flattened into length 784 vectors). Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1:

```{r}
# reshape
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255
```

Note that we use the array_reshape() function rather than the dim<-() function to reshape the array. This is so that the data is re-interpreted using row-major semantics (as opposed to R’s default column-major semantics), which is in turn compatible with the way that the numerical libraries called by Keras interpret array dimensions.

The y data is an integer vector with values ranging from 0 to 9. To prepare this data for training we one-hot encode the vectors into binary class matrices using the Keras to_categorical() function:

```{r}
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

### DEFINING THE MODEL
The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers.

We begin by creating a sequential model and then adding layers using the pipe (%>%) operator:

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')
```

The input_shape argument to the first layer specifies the shape of the input data (a length 784 numeric vector representing a grayscale image). The final layer outputs a length 10 numeric vector (probabilities for each digit) using a softmax activation function.

Use the summary() function to print the details of the model:

```{r}
summary(model)
```

Next, compile the model with appropriate loss function, optimizer, and metrics:

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

### TRAINING AND EVALUATION
Use the fit() function to train the model for 30 epochs using batches of 128 images:

```{r}
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

The history object returned by fit() includes loss and accuracy metrics which we can plot:

```{r}
plot(history)
```

Evaluate the model’s performance on the test data:

```{r}
model %>% evaluate(x_test, y_test)
```

Generate predictions on new data:
```{r}
model %>% predict_classes(x_test)
```

Keras provides a vocabulary for building deep learning models that is simple, elegant, and intuitive. Building a question answering system, an image classification model, a neural Turing machine, or any other model is just as straightforward.

The above is a toy example, which just gives you some sense of deep learning. You can see the difference between training a traditional ML model and training a deep learning model.

**The END**
