{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Esc` or clicking the left blue bar enters the **command mode**\n",
    "* `Shift+Enter` to run cells\n",
    "* `A` adds a cell above\n",
    "* `B` adds a cell below\n",
    "* `D`, `D` (press twice) deletes cells\n",
    "\n",
    "* `M` converts to markdown cells\n",
    "* `Y` converts to code cells\n",
    "\n",
    "* `X` cuts cells\n",
    "* `C` copies cells\n",
    "* `V` pastes cells\n",
    "* `Z` undo\n",
    "\n",
    "Find out more on https://towardsdatascience.com/jypyter-notebook-shortcuts-bf0101a98330."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory\n",
    "import os\n",
    "path=os.getcwd()\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder for our course\n",
    "new_path=\"./soc591/\"\n",
    "os.makedirs(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the current working directory to soc591\n",
    "os.chdir(new_path)\n",
    "# check the current wd\n",
    "print(\"Current wd: \",os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create a new file in current WD, write some texts into the file, and then close it\n",
    "f = open(\"soc591.txt\",mode=\"w+\")\n",
    "col_vars = \"id;text\\n\"\n",
    "f.write(col_vars)\n",
    "f.write(\"1;This is a demo for writing some texts\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us read the soc591.txt file and assign it to variable text_df\n",
    "text_df = open(\"soc591.txt\", \"r\").read()\n",
    "print(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list file content\n",
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us remove the soc591.txt file\n",
    "os.remove(\"soc591.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Static Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import urllib.parse\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below partly comes from [Dr. Yongren SHI's](https://clas.uiowa.edu/sociology/people/yongren-shi) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.imdb.com/search/title/?groups=top_250&sort=user_rating,desc\"\n",
    "\n",
    "# Specify userheader\n",
    "userHeader = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/8.0.7 Safari/600.7.12\"}\n",
    "req = urllib.request.Request(url, headers=userHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need a VPN for web scraping\n",
    "# open url and read web page\n",
    "response = urllib.request.urlopen(req)\n",
    "the_page = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# beautifulsoup parse html\n",
    "soup=bs(the_page,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Elements from Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_containers = soup.find_all('div',{'class':'lister-item mode-advanced'})\n",
    "print(movie_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the First Movie\n",
    "first_movie = movie_containers[0]\n",
    "print(first_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie's title\n",
    "movie_name = first_movie.h3.a.text\n",
    "movie_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Loop and List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 2, 3]:\n",
    "    print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i+1 for i in [1, 2, 3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Movies in the First Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_year = first_movie.h3.find('span',{'class':'lister-item-year text-muted unbold'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the year the movie was released\n",
    "import re\n",
    "movie_year = first_movie.h3.find('span',{'class':'lister-item-year text-muted unbold'}).text\n",
    "[int(year) for year in re.findall(\"\\d{4}\", movie_year)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_names = [movie.h3.a.text for movie in movie_containers]\n",
    "movie_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "movie_years = [movie.h3.find('span',{'class':'lister-item-year text-muted unbold'}).text for movie in movie_containers]\n",
    "movie_years = [[int(year) for year in re.findall(\"\\d{4}\", movie_year)][0] for movie_year in movie_years]\n",
    "movie_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_DF = pd.DataFrame({\"name\":movie_names,\"year\":movie_years})\n",
    "movie_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Top 50 Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_numbers=list(range(1, 202, 50))\n",
    "start_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ''.join([]) to join strings\n",
    "url1=\"https://www.imdb.com/search/title/?groups=top_250&sort=user_rating,desc&start=\"\n",
    "url2=\"&ref_=adv_nxt\"\n",
    "''.join([url1, str(1), url2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[''.join([url1, str(i), url2]) for i in start_numbers]\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_movie(url):\n",
    "    userHeader = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/8.0.7 Safari/600.7.12\"}\n",
    "    req = urllib.request.Request(url, headers=userHeader)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    the_page = response.read()\n",
    "    soup=bs(the_page,\"html.parser\")\n",
    "    movie_containers = soup.find_all('div',{'class':'lister-item mode-advanced'})\n",
    "    return movie_containers\n",
    "\n",
    "movie_containers_all = [scrape_movie(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_containers_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_containers_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested list comprehension: [f(i) for j in k for i in j]\n",
    "movie_names = [movie.h3.a.text for movie_containers in movie_containers_all for movie in movie_containers]\n",
    "movie_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(movie_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested list comprehension: [f(i) for j in k for i in j]\n",
    "movie_years = [movie.h3.find('span',{'class':'lister-item-year text-muted unbold'}).text \n",
    "for movie_containers in movie_containers_all for movie in movie_containers]\n",
    "movie_years = [[int(year) for year in re.findall(\"\\d{4}\", movie_year)][0] for movie_year in movie_years]\n",
    "movie_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_DF = pd.DataFrame({\"name\":movie_names,\"year\":movie_years})\n",
    "movie_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may not run this!\n",
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules for use\n",
    "import os\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Driver\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "# Open the url and establish a connection\n",
    "url = \"https://elephrame.com/textbook/BLM/chart\"\n",
    "driver.implicitly_wait(5)\n",
    "driver.maximize_window()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down to the bottom of the page\n",
    "#driver.execute_script(\"window.scrollTo(0,window.scrollY+300)\")\n",
    "driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "\n",
    "# Read and parse the first page\n",
    "first_page = driver.page_source\n",
    "first_page\n",
    "\n",
    "soup = bs(first_page,\"html.parser\")\n",
    "\n",
    "# Use google developer inspect to check the source codes\n",
    "# locate the key info we need\n",
    "# it stores ad div class = \"item chart\"\n",
    "items = soup.find_all(\"div\",{\"class\":\"item chart\"})\n",
    "print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protest_id = [re.findall(r'id=\"([0-9].*?)\"',str(item))[0] for item in items]\n",
    "protest_location = [''.join(item.find(\"div\",{\"class\":\"item-protest-location\"}).text.split()) for item in items]\n",
    "protest_start = [''.join(item.find(\"div\",{\"class\":\"protest-start\"}).text.split()) for item in items]\n",
    "protest_end = [''.join(item.find(\"div\",{\"class\":\"protest-end\"}).text.split()) for item in items]\n",
    "protest_subject = [''.join(item.find(\"li\",{\"class\":\"item-protest-subject\"}).text.split()) for item in items]\n",
    "protest_participants = [''.join(item.find(\"li\",{\"class\":\"item-protest-participants\"}).text.split()) for item in items]\n",
    "protest_time = [''.join(item.find(\"li\",{\"class\":\"item-protest-time\"}).text.split()) for item in items]\n",
    "protest_description = [''.join(item.find(\"li\",{\"class\":\"item-protest-description\"}).text.split()) for item in items]\n",
    "protest_urls = ['##'.join(item.find(\"li\",{\"class\":\"item-protest-url\"}).text.split()) for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the last item content into a tsv file for check\n",
    "# check current dir\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to a csv file\n",
    "import csv \n",
    "from itertools import zip_longest\n",
    "data=[protest_id, protest_location,protest_start,protest_end,protest_subject,protest_participants, \n",
    "protest_time,protest_description, protest_urls]\n",
    "export_data = zip_longest(*data, fillvalue = '')\n",
    "with open('blm-data.csv', 'w', encoding=\"ISO-8859-1\", newline='') as file:\n",
    "      write = csv.writer(file)\n",
    "      write.writerow((\"protest_id\", \"protest_location\",\"protest_start\",\"protest_end\",\"protest_subject\",\"protest_participants\", \n",
    "      \"protest_time\",\"protest_description\", \"protest_urls\"))\n",
    "      write.writerows(export_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click the next page\n",
    "# you can check here for more info on selenium how to locate elements \n",
    "# https://selenium-python.readthedocs.io/locating-elements.html\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "next_page = driver.find_element(By.XPATH, '//div[@class=\"pagination\"]//li[4]')\n",
    "next_page.click()\n",
    "time.sleep(5)\n",
    "# then we repeat the process to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we have 229 pages, so we need a loop to automate the process\n",
    "soup = bs(driver.page_source,\"html.parser\")\n",
    "# locate the page id\n",
    "page_id = soup.find(\"input\",{\"class\":\"page-choice\"})[\"value\"]\n",
    "page_id = int(page_id)\n",
    "print(page_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Please check the number of pages on your computer.\n",
    "while page_id <=312:\n",
    "    # do first page scraping \n",
    "    # click next page\n",
    "    # repeat the scraping\n",
    "    # if page_id>312, then stop\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.tripadvisor.com/Attraction_Review-g187323-d617423-Reviews-The_Holocaust_Memorial_Memorial_to_the_Murdered_Jews_of_Europe-Berlin.html\"\n",
    "\n",
    "# Specify userheader\n",
    "userHeader = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/600.7.12 (KHTML, like Gecko) Version/8.0.7 Safari/600.7.12\"}\n",
    "req = urllib.request.Request(url, headers=userHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open url and read web page\n",
    "response = urllib.request.urlopen(req)\n",
    "the_page = response.read()\n",
    "\n",
    "# beautifulsoup parse html\n",
    "soup=bs(the_page,\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to follow codes above to solve the challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "398dc28c06ad810e77de546bbdfa897a6ee0b83e59a5207339dda01a7843e01d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
